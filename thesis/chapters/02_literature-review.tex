\chapter{\todoorange{Literature Review}}
\label{chap:lr}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% - Identify groups of related work (e.g. variations of an algorithm, implementations, theoretical results, etc.)
% - This section may include papers (published research, pre-prints), existing libraries, software, blog posts, etc.
% - You should not copy-paste abstract of a reviewed paper. Instead, you must read it carefully and identify the key points of this work that relate to your thesis.
% - To cover:
%   - системы типов и HM
%   - раздел про free-foil с описанием (главное: про что он и как им пользоваться)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{\todogreen{Hindley-Milner Type System}}

In 1969, Hindley discovered an approach of computing a principal type scheme for a given object in combinatory logic \cite{Hindley1969_PrincipalTypeScheme}. Later, in 1978, Milner introduced two algorithms for inferring polymorphic type for a given program: algorithm $\mathcal{W}$ and $\mathcal{J}$, and proved soundness of $\mathcal{W}$ \cite{Milner1978_TypePolymorphism}. Finally, in 1984 \cite{Damas1984_TypeAssignment}, Damas proved completeness of $\mathcal{W}$ and that $\mathcal{W}$ indeed infers the principal type of a given term. Mentioned works formed the theoretical basis for the family of type systems, in the literature khown as variations of Hindley-Milner (HM) type system.

Hindley's "principal type scheme" and Milner's "polymorphic type" are about the same concept — the most general type of a term, from which all other types can be instantiated. For example, the identity function $\lambda x. x$ can be typed as $\texttt{Int} \to \texttt{Int}$, or $\texttt{Bool} \to \texttt{Bool}$, but its most general type is $\forall \alpha. \alpha \to \alpha$.

HM forms the foundation of modern functional programming languages such as ML, OCaml, and Haskell. It is widely adopted due to its relative simplicity (variant of typing rules is shown on Figure~\ref{fig:syntactical-hm-rules}), formal properties of completeness and soundness, and its support for parametric polymorphism, which enables the inference of principal types for terms without requiring explicit type annotations, making the system both expressive and practical for use.

\begin{figure}[H]
  \fbox{\begin{minipage}[c][\height+1.5cm][t]{\dimexpr\textwidth-2\fboxsep-2\fboxrule}
    \begin{mathpar}
      \inferrule
        {x : \sigma \in \Gamma \\
          \sigma \sqsubseteq \tau}
        {\Gamma \vdash x : \tau}
        \,[\texttt{Var}]

      \inferrule
        {\Gamma \vdash e_0 : \tau \to \tau' \\
          \Gamma \vdash e_1 : \tau}
        {\Gamma \vdash e_0\;e_1 : \tau'}
        \,[\texttt{App}]

      \inferrule
        {\Gamma, x : \tau \vdash e : \tau'}
        {\Gamma \vdash \lambda x. e : \tau \to \tau'}
        \,[\texttt{Abs}]

      \inferrule
        {\Gamma \vdash e_0 : \tau \\
        \Gamma, x : \overline{\Gamma}(\tau) \vdash e_1 : \tau'}
        {\Gamma \vdash \texttt{let } x = e_0 \texttt{ in } e_1 : \tau'}
        \,[\texttt{Let}]
    \end{mathpar}
  \end{minipage}}
  \caption{Syntactical Hindley-Milner system rules \cite{Clement1986_MiniML}}
  \label{fig:syntactical-hm-rules}
\end{figure}

\begin{figure}[H]
  \fbox{\begin{minipage}[c][\height+0.5cm][c]{\dimexpr\textwidth-2\fboxsep-2\fboxrule}
    \vspace*{-1cm}
    \begin{mathpar}
      \overline{\Gamma}(\tau) = \forall \hat{\alpha}.\tau

      \hat{\alpha} = \texttt{free}(\tau) \setminus \texttt{free}(\Gamma)  
    \end{mathpar}
  \end{minipage}}
  \caption{Generalization of a type $\tau$ under context $\Gamma$}
  \label{fig:generalization-def}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{\todogreen{Levels}}

In HM type system, let-polymorphism enables the definition of polymorphic functions through let-expressions. The process involves two primary steps: generalization and specialization (or instantiation). Generalization quantifies free type variables in the monotype of a let-bound expression to form a polytype, while specialization instantiates these quantified variables with fresh ones at each use of the bound variable.

However, generalizing type variables introduced outside the let binding can lead to unsound type checking. Consider the expression:

$$
\lambda x.\ \texttt{let}\ y = (\lambda z.\ x)\ \texttt{in}\ y
$$

Here, the type of expression bound by $y$ is inferred as $A \to B$, where $B$ and $A$ are the types of $x$ and $z$ accordingly. Generalizing both $A$ and $B$ to form $\forall \alpha. \forall \beta.\ \alpha \to \beta$ would be incorrect, as $\beta$ is bound in the outer context. The correct type for $y$ is $\forall \alpha.\ \alpha \to B$, ensuring that only $\alpha$, introduced within the expression bound by let, is generalized.

Traditional type inference algorithms, such as $\mathcal{W}$~and~$\mathcal{J}$~\cite{Milner1978_TypePolymorphism}, perform generalization by computing the free type variables in the inferred type and subtracting those that are free in the type environment (see Figure~\ref{fig:generalization-def}). This process requires scanning both the type and the entire environment. To address this inefficiency, Didier R\'emy introduced the concept of ranks (or levels) in his work~\cite{Remy1992_SortedEqTheoryTypes}. As pointed out by R\'emy, this inefficiency is not particularly critical in the standard HM, but becomes important when new extensions, such as subtyping, are introduced.

Levels are integers representing the nesting depth of let-expressions. Each type variable is associated with a level corresponding to the let-expression in which it was introduced, with level 1 assigned to the implicit top-level let of a program. Level assigned to a unification variable may change after it was introduced. During unification, if a type variable is unified with a type at a lower level, its level is updated to the minimum of the two, ensuring it reflects the outermost scope in which it is used~\cite{Kiselyov2022_OCamplTypeChecker}. Finally, when generalizing a let-bound expression at level $n$, only free variables of the type with levels greater than $n$ are quantified. As a result, sound generalization is preserved without "touching" a typing environment at all.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{\todogreen{Name Management}}

An integral feature of any programming language or formal system is an ability to work with bound names (or local variables), e.g. a parameter of a $\lambda$-abstraction. In the example below, $x$ is said to be \textit{bound} by a \textit{binder} $\lambda x.$, and $y$ is said to be \textit{free} as it does not have a binder:

$$
\lambda x." "x" "y
$$

The main pain point of name management is scoping, that is keeping track of which names are bound by which binders in order to resolve names correctly avoiding collisions. The common operation of name management is substitution, that is, replacing the occurances of a variable with another expression. We may want to replace the occurances of the variable bound by $\lambda$-abstraction, in order to reduce the expression. Or, during type inference, when specializing a polymorphic type in let expression, it is necessary to substitute quantified type variables with the fresh ones. Both these tasks, as well as many others, require a substitution. However, implementing a substitution is not trivial at all.

To illustrate why substitution is not an easy task, consider the process of reduction of this $\lambda$-expression:

\begin{align*}
  & (\lambda x. \lambda y. x) y \\
  & \rightarrow \text{($\beta$-reduction)} \\
  & [x \mapsto y](\lambda y. x) \\
  & \rightarrow \text{(substitution)} \\
  & \textcolor{red}{\lambda y. y}
\end{align*}

We got an identity function, which is wrong, since $y$ in the body of $\lambda$-abstraction is not the same $y$ as before, it has been \textit{captured}. Naïve substitution does not work in this case. Instead, we need a \textit{capture-avoiding substitution}.

Capture-avoiding substitution is an old and well-studied problem in computer science, with extensive research done as well as numerous practical implementations applied in real-world projects.

De~Bruijn indices~\cite{deBruijn1972} is one of the earliest and simplest approaches to capture-avoiding substitution, which suggests to replace bound names with natural numbers representing the position relative to their binder. However, such nameless representation is not efficient in practice due to often shifts and expressions traversal. In recent decades, more efficient approaches to capture-avoiding substitution have emerged. One notable example is the stateless "rapier" technique~\cite{Simon2002_SecretsGHC}, which is used by the Glasgow Haskell Compiler.

However, besides efficency, it is worth considering another two important aspects when choosing between different name management techniques, which are intrinsic scoping and generality.

\section{\todoorange{Intrinsic Scoping}}

Intrinsically scoped, or scope-safe, abstract syntax representation of the language terms helps to minimize a chance of introducing bugs, by leveraging the power of the host language type system. For example, in 1999 Bird and Paterson \cite{BirdPaterson1999_BruijnNested} showed how invariants of the de Bruijn notation can be described on the type level by using nested datatypes \cite{Bird1998_NestedDatatypes}. Dex programming language~\cite{PaszkeDex_2021} is another illustrative example why using scope-safe abstract syntax is beneficial. When adopting the mentioned "rapier" technique in Dex, Google research team faced many difficulties, which led them to develop the foil~\cite{Foil} — an enhancement that leverages the Haskell's type system to maintain rapier's invariants.

<TODO: explain how scope-safety is achieved in foil>

\section{\todoorange{Generic Abstract Syntax}}

Generality is another possible characteristic of the abstract syntax, which helps to avoid repeatition and code duplication. With generic abstract syntax it becomes possible to reuse already implemented and tested algorithms, as well as to easily extend existing code. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{\todoorange{Free Foil}}

Free Foil introduced by Kudasov \textit{et al.} \cite{FreeFoil}, is a framework that generates efficient, scope-safe abstract syntax, enabling users to effortlessly add support of name management into an object language.

...

<TODO: demonstrate main data types and ideas of free foil>

...

Scope-safe generic abstract syntax can be generated with Free Foil in two ways: by using free scoped monads or via Template Haskell \cite{SheardPeytonJones2002_TH} GHC extension (i.e. metaprogramming). In this work we have chosen the second approach, as it requires writing less code and, more importantly, integrates nicely with BNFC.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{\todogreen{Code Generation Tools}}

Backus–Naur Form Converter (BNFC) \cite{BNFC} is a tool for generating compiler front-end for an object language, given its' grammar in Labelled BNF \cite{BackusNaurForm2003}. By assigning a label to each rule in the grammar, we tell the BNFC what constructors to use when generating the syntax tree in the host language.

Figure~\ref{fig:lbnf-bnfc-example} below illustrates how BNF rules and their labels defined in the grammar file correspond to the Haskell code generated by BNFC. Different rules with the same label correspond to the different constructors of the same type. In the generated code, terminals from the grammar (such as \texttt{"true"} in the \texttt{ETrue} rule) are omitted, while non-terminals (such as \texttt{Exp1} and \texttt{Exp2} in the \texttt{EAdd} rule) become parameters of the corresponding type constructor. Lastly, integer postfix in the name of each rule only serves as the precedence level of this rule, and is ommitted in the resulting type name. Thus, both \texttt{Exp1} and \texttt{Exp2} are merged to \texttt{Exp} in code.

\begin{figure}[H]
  \centering
  \begin{minipage}{0.49\textwidth}
    \begin{minted}[frame=single,fontsize=\small]{text}
EVar.    Exp2 ::= Ident ;
ETrue.   Exp2 ::= "true" ;
EFalse.  Exp2 ::= "false" ;
ENat.    Exp2 ::= Integer ;
EAdd.    Exp1 ::= Exp1 "+" Exp2 ;
    \end{minted}
    \hfill
  \end{minipage}
  \begin{minipage}{0.49\textwidth}
    \begin{minted}[frame=single,fontsize=\small]{haskell}
data Exp
  = EVar Ident
  | ETrue 
  | EFalse
  | ENat Integer
  | EAdd Exp Exp
    \end{minted}
  \end{minipage}
  \caption[Input grammar and code produced by BNFC]{Example of the LBNF grammar and corresponding data type in Haskell, generated by BNFC.}
  \label{fig:lbnf-bnfc-example}
\end{figure}

BNFC significantly simplifies the process of prototyping a compiler, or a type checker for an object language. By supplying only the language grammar to BNFC, user gets a handful of functionality generated automatically, such as abstract syntax with pretty-printing, and, more importantly, specifications for lexer and parser generators. In fact, to further generate code of the lexer and parser, additional language-specific tools are required. And since the focus of this work is to implement a type checker in \textit{Haskell}, we use BNFC together with Alex \cite{haskell_alex}, a lexer generator, and Happy \cite{haskell_happy}, a parser generator.
